{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4NcxHGwkoqr+CX3agS9Ji",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EllaElect/Deep-learning-Project/blob/main/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fZw6CFU4BqUd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2b082b02-4399-49df-e504-16fa28ef374d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
            "       [4.9, 3. , 1.4, 0.2],\n",
            "       [4.7, 3.2, 1.3, 0.2],\n",
            "       [4.6, 3.1, 1.5, 0.2],\n",
            "       [5. , 3.6, 1.4, 0.2],\n",
            "       [5.4, 3.9, 1.7, 0.4],\n",
            "       [4.6, 3.4, 1.4, 0.3],\n",
            "       [5. , 3.4, 1.5, 0.2],\n",
            "       [4.4, 2.9, 1.4, 0.2],\n",
            "       [4.9, 3.1, 1.5, 0.1],\n",
            "       [5.4, 3.7, 1.5, 0.2],\n",
            "       [4.8, 3.4, 1.6, 0.2],\n",
            "       [4.8, 3. , 1.4, 0.1],\n",
            "       [4.3, 3. , 1.1, 0.1],\n",
            "       [5.8, 4. , 1.2, 0.2],\n",
            "       [5.7, 4.4, 1.5, 0.4],\n",
            "       [5.4, 3.9, 1.3, 0.4],\n",
            "       [5.1, 3.5, 1.4, 0.3],\n",
            "       [5.7, 3.8, 1.7, 0.3],\n",
            "       [5.1, 3.8, 1.5, 0.3],\n",
            "       [5.4, 3.4, 1.7, 0.2],\n",
            "       [5.1, 3.7, 1.5, 0.4],\n",
            "       [4.6, 3.6, 1. , 0.2],\n",
            "       [5.1, 3.3, 1.7, 0.5],\n",
            "       [4.8, 3.4, 1.9, 0.2],\n",
            "       [5. , 3. , 1.6, 0.2],\n",
            "       [5. , 3.4, 1.6, 0.4],\n",
            "       [5.2, 3.5, 1.5, 0.2],\n",
            "       [5.2, 3.4, 1.4, 0.2],\n",
            "       [4.7, 3.2, 1.6, 0.2],\n",
            "       [4.8, 3.1, 1.6, 0.2],\n",
            "       [5.4, 3.4, 1.5, 0.4],\n",
            "       [5.2, 4.1, 1.5, 0.1],\n",
            "       [5.5, 4.2, 1.4, 0.2],\n",
            "       [4.9, 3.1, 1.5, 0.2],\n",
            "       [5. , 3.2, 1.2, 0.2],\n",
            "       [5.5, 3.5, 1.3, 0.2],\n",
            "       [4.9, 3.6, 1.4, 0.1],\n",
            "       [4.4, 3. , 1.3, 0.2],\n",
            "       [5.1, 3.4, 1.5, 0.2],\n",
            "       [5. , 3.5, 1.3, 0.3],\n",
            "       [4.5, 2.3, 1.3, 0.3],\n",
            "       [4.4, 3.2, 1.3, 0.2],\n",
            "       [5. , 3.5, 1.6, 0.6],\n",
            "       [5.1, 3.8, 1.9, 0.4],\n",
            "       [4.8, 3. , 1.4, 0.3],\n",
            "       [5.1, 3.8, 1.6, 0.2],\n",
            "       [4.6, 3.2, 1.4, 0.2],\n",
            "       [5.3, 3.7, 1.5, 0.2],\n",
            "       [5. , 3.3, 1.4, 0.2],\n",
            "       [7. , 3.2, 4.7, 1.4],\n",
            "       [6.4, 3.2, 4.5, 1.5],\n",
            "       [6.9, 3.1, 4.9, 1.5],\n",
            "       [5.5, 2.3, 4. , 1.3],\n",
            "       [6.5, 2.8, 4.6, 1.5],\n",
            "       [5.7, 2.8, 4.5, 1.3],\n",
            "       [6.3, 3.3, 4.7, 1.6],\n",
            "       [4.9, 2.4, 3.3, 1. ],\n",
            "       [6.6, 2.9, 4.6, 1.3],\n",
            "       [5.2, 2.7, 3.9, 1.4],\n",
            "       [5. , 2. , 3.5, 1. ],\n",
            "       [5.9, 3. , 4.2, 1.5],\n",
            "       [6. , 2.2, 4. , 1. ],\n",
            "       [6.1, 2.9, 4.7, 1.4],\n",
            "       [5.6, 2.9, 3.6, 1.3],\n",
            "       [6.7, 3.1, 4.4, 1.4],\n",
            "       [5.6, 3. , 4.5, 1.5],\n",
            "       [5.8, 2.7, 4.1, 1. ],\n",
            "       [6.2, 2.2, 4.5, 1.5],\n",
            "       [5.6, 2.5, 3.9, 1.1],\n",
            "       [5.9, 3.2, 4.8, 1.8],\n",
            "       [6.1, 2.8, 4. , 1.3],\n",
            "       [6.3, 2.5, 4.9, 1.5],\n",
            "       [6.1, 2.8, 4.7, 1.2],\n",
            "       [6.4, 2.9, 4.3, 1.3],\n",
            "       [6.6, 3. , 4.4, 1.4],\n",
            "       [6.8, 2.8, 4.8, 1.4],\n",
            "       [6.7, 3. , 5. , 1.7],\n",
            "       [6. , 2.9, 4.5, 1.5],\n",
            "       [5.7, 2.6, 3.5, 1. ],\n",
            "       [5.5, 2.4, 3.8, 1.1],\n",
            "       [5.5, 2.4, 3.7, 1. ],\n",
            "       [5.8, 2.7, 3.9, 1.2],\n",
            "       [6. , 2.7, 5.1, 1.6],\n",
            "       [5.4, 3. , 4.5, 1.5],\n",
            "       [6. , 3.4, 4.5, 1.6],\n",
            "       [6.7, 3.1, 4.7, 1.5],\n",
            "       [6.3, 2.3, 4.4, 1.3],\n",
            "       [5.6, 3. , 4.1, 1.3],\n",
            "       [5.5, 2.5, 4. , 1.3],\n",
            "       [5.5, 2.6, 4.4, 1.2],\n",
            "       [6.1, 3. , 4.6, 1.4],\n",
            "       [5.8, 2.6, 4. , 1.2],\n",
            "       [5. , 2.3, 3.3, 1. ],\n",
            "       [5.6, 2.7, 4.2, 1.3],\n",
            "       [5.7, 3. , 4.2, 1.2],\n",
            "       [5.7, 2.9, 4.2, 1.3],\n",
            "       [6.2, 2.9, 4.3, 1.3],\n",
            "       [5.1, 2.5, 3. , 1.1],\n",
            "       [5.7, 2.8, 4.1, 1.3],\n",
            "       [6.3, 3.3, 6. , 2.5],\n",
            "       [5.8, 2.7, 5.1, 1.9],\n",
            "       [7.1, 3. , 5.9, 2.1],\n",
            "       [6.3, 2.9, 5.6, 1.8],\n",
            "       [6.5, 3. , 5.8, 2.2],\n",
            "       [7.6, 3. , 6.6, 2.1],\n",
            "       [4.9, 2.5, 4.5, 1.7],\n",
            "       [7.3, 2.9, 6.3, 1.8],\n",
            "       [6.7, 2.5, 5.8, 1.8],\n",
            "       [7.2, 3.6, 6.1, 2.5],\n",
            "       [6.5, 3.2, 5.1, 2. ],\n",
            "       [6.4, 2.7, 5.3, 1.9],\n",
            "       [6.8, 3. , 5.5, 2.1],\n",
            "       [5.7, 2.5, 5. , 2. ],\n",
            "       [5.8, 2.8, 5.1, 2.4],\n",
            "       [6.4, 3.2, 5.3, 2.3],\n",
            "       [6.5, 3. , 5.5, 1.8],\n",
            "       [7.7, 3.8, 6.7, 2.2],\n",
            "       [7.7, 2.6, 6.9, 2.3],\n",
            "       [6. , 2.2, 5. , 1.5],\n",
            "       [6.9, 3.2, 5.7, 2.3],\n",
            "       [5.6, 2.8, 4.9, 2. ],\n",
            "       [7.7, 2.8, 6.7, 2. ],\n",
            "       [6.3, 2.7, 4.9, 1.8],\n",
            "       [6.7, 3.3, 5.7, 2.1],\n",
            "       [7.2, 3.2, 6. , 1.8],\n",
            "       [6.2, 2.8, 4.8, 1.8],\n",
            "       [6.1, 3. , 4.9, 1.8],\n",
            "       [6.4, 2.8, 5.6, 2.1],\n",
            "       [7.2, 3. , 5.8, 1.6],\n",
            "       [7.4, 2.8, 6.1, 1.9],\n",
            "       [7.9, 3.8, 6.4, 2. ],\n",
            "       [6.4, 2.8, 5.6, 2.2],\n",
            "       [6.3, 2.8, 5.1, 1.5],\n",
            "       [6.1, 2.6, 5.6, 1.4],\n",
            "       [7.7, 3. , 6.1, 2.3],\n",
            "       [6.3, 3.4, 5.6, 2.4],\n",
            "       [6.4, 3.1, 5.5, 1.8],\n",
            "       [6. , 3. , 4.8, 1.8],\n",
            "       [6.9, 3.1, 5.4, 2.1],\n",
            "       [6.7, 3.1, 5.6, 2.4],\n",
            "       [6.9, 3.1, 5.1, 2.3],\n",
            "       [5.8, 2.7, 5.1, 1.9],\n",
            "       [6.8, 3.2, 5.9, 2.3],\n",
            "       [6.7, 3.3, 5.7, 2.5],\n",
            "       [6.7, 3. , 5.2, 2.3],\n",
            "       [6.3, 2.5, 5. , 1.9],\n",
            "       [6.5, 3. , 5.2, 2. ],\n",
            "       [6.2, 3.4, 5.4, 2.3],\n",
            "       [5.9, 3. , 5.1, 1.8]]), 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), 'frame': None, 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'), 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...', 'feature_names': ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'], 'filename': 'iris.csv', 'data_module': 'sklearn.datasets.data'}\n",
            "X [[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [4.6 3.1 1.5 0.2]\n",
            " [5.  3.6 1.4 0.2]\n",
            " [5.4 3.9 1.7 0.4]\n",
            " [4.6 3.4 1.4 0.3]\n",
            " [5.  3.4 1.5 0.2]\n",
            " [4.4 2.9 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [5.4 3.7 1.5 0.2]\n",
            " [4.8 3.4 1.6 0.2]\n",
            " [4.8 3.  1.4 0.1]\n",
            " [4.3 3.  1.1 0.1]\n",
            " [5.8 4.  1.2 0.2]\n",
            " [5.7 4.4 1.5 0.4]\n",
            " [5.4 3.9 1.3 0.4]\n",
            " [5.1 3.5 1.4 0.3]\n",
            " [5.7 3.8 1.7 0.3]\n",
            " [5.1 3.8 1.5 0.3]\n",
            " [5.4 3.4 1.7 0.2]\n",
            " [5.1 3.7 1.5 0.4]\n",
            " [4.6 3.6 1.  0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.8 3.4 1.9 0.2]\n",
            " [5.  3.  1.6 0.2]\n",
            " [5.  3.4 1.6 0.4]\n",
            " [5.2 3.5 1.5 0.2]\n",
            " [5.2 3.4 1.4 0.2]\n",
            " [4.7 3.2 1.6 0.2]\n",
            " [4.8 3.1 1.6 0.2]\n",
            " [5.4 3.4 1.5 0.4]\n",
            " [5.2 4.1 1.5 0.1]\n",
            " [5.5 4.2 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.2]\n",
            " [5.  3.2 1.2 0.2]\n",
            " [5.5 3.5 1.3 0.2]\n",
            " [4.9 3.6 1.4 0.1]\n",
            " [4.4 3.  1.3 0.2]\n",
            " [5.1 3.4 1.5 0.2]\n",
            " [5.  3.5 1.3 0.3]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [4.4 3.2 1.3 0.2]\n",
            " [5.  3.5 1.6 0.6]\n",
            " [5.1 3.8 1.9 0.4]\n",
            " [4.8 3.  1.4 0.3]\n",
            " [5.1 3.8 1.6 0.2]\n",
            " [4.6 3.2 1.4 0.2]\n",
            " [5.3 3.7 1.5 0.2]\n",
            " [5.  3.3 1.4 0.2]\n",
            " [7.  3.2 4.7 1.4]\n",
            " [6.4 3.2 4.5 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [5.5 2.3 4.  1.3]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [5.7 2.8 4.5 1.3]\n",
            " [6.3 3.3 4.7 1.6]\n",
            " [4.9 2.4 3.3 1. ]\n",
            " [6.6 2.9 4.6 1.3]\n",
            " [5.2 2.7 3.9 1.4]\n",
            " [5.  2.  3.5 1. ]\n",
            " [5.9 3.  4.2 1.5]\n",
            " [6.  2.2 4.  1. ]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [5.6 2.9 3.6 1.3]\n",
            " [6.7 3.1 4.4 1.4]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.8 2.7 4.1 1. ]\n",
            " [6.2 2.2 4.5 1.5]\n",
            " [5.6 2.5 3.9 1.1]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [6.1 2.8 4.  1.3]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.8 4.7 1.2]\n",
            " [6.4 2.9 4.3 1.3]\n",
            " [6.6 3.  4.4 1.4]\n",
            " [6.8 2.8 4.8 1.4]\n",
            " [6.7 3.  5.  1.7]\n",
            " [6.  2.9 4.5 1.5]\n",
            " [5.7 2.6 3.5 1. ]\n",
            " [5.5 2.4 3.8 1.1]\n",
            " [5.5 2.4 3.7 1. ]\n",
            " [5.8 2.7 3.9 1.2]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.  3.4 4.5 1.6]\n",
            " [6.7 3.1 4.7 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [5.6 3.  4.1 1.3]\n",
            " [5.5 2.5 4.  1.3]\n",
            " [5.5 2.6 4.4 1.2]\n",
            " [6.1 3.  4.6 1.4]\n",
            " [5.8 2.6 4.  1.2]\n",
            " [5.  2.3 3.3 1. ]\n",
            " [5.6 2.7 4.2 1.3]\n",
            " [5.7 3.  4.2 1.2]\n",
            " [5.7 2.9 4.2 1.3]\n",
            " [6.2 2.9 4.3 1.3]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [5.7 2.8 4.1 1.3]\n",
            " [6.3 3.3 6.  2.5]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [7.1 3.  5.9 2.1]\n",
            " [6.3 2.9 5.6 1.8]\n",
            " [6.5 3.  5.8 2.2]\n",
            " [7.6 3.  6.6 2.1]\n",
            " [4.9 2.5 4.5 1.7]\n",
            " [7.3 2.9 6.3 1.8]\n",
            " [6.7 2.5 5.8 1.8]\n",
            " [7.2 3.6 6.1 2.5]\n",
            " [6.5 3.2 5.1 2. ]\n",
            " [6.4 2.7 5.3 1.9]\n",
            " [6.8 3.  5.5 2.1]\n",
            " [5.7 2.5 5.  2. ]\n",
            " [5.8 2.8 5.1 2.4]\n",
            " [6.4 3.2 5.3 2.3]\n",
            " [6.5 3.  5.5 1.8]\n",
            " [7.7 3.8 6.7 2.2]\n",
            " [7.7 2.6 6.9 2.3]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.9 3.2 5.7 2.3]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [7.7 2.8 6.7 2. ]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.7 3.3 5.7 2.1]\n",
            " [7.2 3.2 6.  1.8]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.1 3.  4.9 1.8]\n",
            " [6.4 2.8 5.6 2.1]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [7.4 2.8 6.1 1.9]\n",
            " [7.9 3.8 6.4 2. ]\n",
            " [6.4 2.8 5.6 2.2]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.1 2.6 5.6 1.4]\n",
            " [7.7 3.  6.1 2.3]\n",
            " [6.3 3.4 5.6 2.4]\n",
            " [6.4 3.1 5.5 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.9 3.1 5.4 2.1]\n",
            " [6.7 3.1 5.6 2.4]\n",
            " [6.9 3.1 5.1 2.3]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [6.8 3.2 5.9 2.3]\n",
            " [6.7 3.3 5.7 2.5]\n",
            " [6.7 3.  5.2 2.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [6.2 3.4 5.4 2.3]\n",
            " [5.9 3.  5.1 1.8]]\n",
            "y [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2]\n",
            "IrisNet(\n",
            "  (fc1): Linear(in_features=4, out_features=100, bias=True)\n",
            "  (relu1): ReLU()\n",
            "  (fc2): Linear(in_features=100, out_features=50, bias=True)\n",
            "  (relu2): ReLU()\n",
            "  (fc3): Linear(in_features=50, out_features=3, bias=True)\n",
            ")\n",
            "loss tensor(1.0957, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(1.0858, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(1.0765, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(1.0676, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(1.0592, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(1.0512, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(1.0435, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(1.0361, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(1.0295, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(1.0237, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(1.0184, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(1.0133, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(1.0083, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(1.0034, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.9985, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.9937, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.9888, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.9840, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.9792, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.9744, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.9696, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.9648, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.9599, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.9551, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.9503, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.9454, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.9406, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.9357, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.9308, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.9260, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.9211, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.9162, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.9113, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.9064, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.9015, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.8966, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.8917, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.8868, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.8819, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.8770, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.8721, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.8672, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.8623, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.8574, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.8526, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.8477, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.8428, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.8379, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.8331, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.8282, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.8233, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.8184, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.8135, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.8087, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.8038, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.7990, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.7941, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.7893, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.7845, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.7797, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.7749, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.7701, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.7654, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.7607, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.7560, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.7513, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.7466, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.7420, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.7374, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.7328, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.7282, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.7237, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.7192, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.7147, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.7103, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.7059, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.7016, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.6972, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.6930, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.6887, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.6846, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.6804, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.6763, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.6723, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.6683, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.6643, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.6603, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.6564, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.6526, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.6488, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.6450, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.6412, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.6375, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.6339, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.6303, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.6267, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.6231, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.6196, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.6162, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.6128, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.6094, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.6061, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.6028, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.5996, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.5963, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.5932, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.5900, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.5869, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.5839, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.5809, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.5779, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.5750, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.5721, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.5692, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.5664, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.5636, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.5608, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.5581, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.5554, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.5528, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.5501, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.5476, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.5450, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.5425, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.5400, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.5375, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.5351, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.5327, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.5303, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.5279, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.5256, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.5233, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.5211, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.5188, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.5166, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.5144, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.5122, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.5101, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.5080, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.5059, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.5038, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.5017, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4997, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4977, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4957, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4937, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4918, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4899, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4879, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4861, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4842, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4823, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4805, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4787, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4769, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4751, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4733, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4716, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4699, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4681, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4664, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4648, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4631, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4614, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4598, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4581, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4565, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4549, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4533, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4517, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4501, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4486, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4470, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4455, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4439, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4424, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4409, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4394, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4379, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4364, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4350, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4335, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4320, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4306, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4292, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4277, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4263, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4249, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4235, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4221, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4207, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4193, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4179, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4165, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4151, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4137, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4123, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4108, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4094, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4079, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4065, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4051, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4038, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4025, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.4012, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3999, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3987, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3974, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3962, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3950, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3937, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3925, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3913, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3901, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3889, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3877, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3866, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3854, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3842, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3830, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3818, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3807, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3795, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3784, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3772, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3761, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3750, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3738, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3727, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3716, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3704, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3693, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3682, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3670, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3659, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3648, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3636, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3625, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3614, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3602, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3591, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3580, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3569, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3558, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3548, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3537, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3526, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3516, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3505, grad_fn=<NllLossBackward0>)\n",
            "loss tensor(0.3495, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3495)\n",
            "1.)tensor([-1.8906,  1.2898,  1.0956])\tVersicolour\t 1\n",
            "2.)tensor([ 3.1809,  0.3120, -3.0149])\tSetosa\t 0\n",
            "3.)tensor([-4.7287,  1.8356,  3.4978])\tVirginica\t 2\n",
            "4.)tensor([-1.8152,  1.2481,  1.0429])\tVersicolour\t 1\n",
            "5.)tensor([-1.7886,  1.3710,  0.9621])\tVersicolour\t 1\n",
            "6.)tensor([ 2.9465,  0.3089, -2.8071])\tSetosa\t 0\n",
            "7.)tensor([-0.6616,  1.0291,  0.0809])\tVersicolour\t 1\n",
            "8.)tensor([-2.5610,  1.4471,  1.6551])\tVirginica\t 2\n",
            "9.)tensor([-2.1909,  1.2922,  1.3568])\tVersicolour\t 2\n",
            "10.)tensor([-1.0377,  1.1231,  0.3760])\tVersicolour\t 1\n",
            "11.)tensor([-2.4964,  1.3977,  1.6152])\tVirginica\t 2\n",
            "12.)tensor([ 2.6943,  0.2703, -2.5848])\tSetosa\t 0\n",
            "13.)tensor([ 3.2846,  0.2434, -3.0681])\tSetosa\t 0\n",
            "14.)tensor([ 2.6968,  0.2922, -2.5933])\tSetosa\t 0\n",
            "15.)tensor([ 3.0941,  0.2371, -2.9058])\tSetosa\t 0\n",
            "16.)tensor([-1.7121,  1.2896,  0.9373])\tVersicolour\t 1\n",
            "17.)tensor([-3.7152,  1.5380,  2.6882])\tVirginica\t 2\n",
            "18.)tensor([-1.2131,  1.1118,  0.5398])\tVersicolour\t 1\n",
            "19.)tensor([-1.9036,  1.2217,  1.1379])\tVersicolour\t 1\n",
            "20.)tensor([-3.6393,  1.5030,  2.6286])\tVirginica\t 2\n",
            "21.)tensor([ 2.5132,  0.3127, -2.4384])\tSetosa\t 0\n",
            "22.)tensor([-2.4391,  1.3315,  1.5894])\tVirginica\t 2\n",
            "23.)tensor([ 2.6694,  0.3208, -2.5697])\tSetosa\t 0\n",
            "24.)tensor([-3.5693,  1.5005,  2.5651])\tVirginica\t 2\n",
            "25.)tensor([-3.0821,  1.7238,  2.0287])\tVirginica\t 2\n",
            "26.)tensor([-2.8732,  1.4510,  1.9391])\tVirginica\t 2\n",
            "27.)tensor([-3.6753,  1.5662,  2.6289])\tVirginica\t 2\n",
            "28.)tensor([-3.6212,  1.5750,  2.5864])\tVirginica\t 2\n",
            "29.)tensor([ 2.5867,  0.2964, -2.4929])\tSetosa\t 0\n",
            "30.)tensor([ 2.4920,  0.3328, -2.4244])\tSetosa\t 0\n",
            "we got 29 correct!\n",
            "Training finished.\n",
            "tensor([ 2.8439,  0.3469, -2.7336])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import torch.nn.functional as f\n",
        "\n",
        "#iris dataset from sklearn\n",
        "#to access and work with the Iris dataset goes here\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "#to load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "print(iris)\n",
        "print( \"X\", X)\n",
        "print(\"y\", y)\n",
        "\n",
        "#convert data to PyTorch tensors\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define custom Dataset class\n",
        "class IrisDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "#creating the module\n",
        "#this can be done in two methods, the init and the forward method\n",
        "#defining the neural network class\n",
        "class IrisNet(nn.Module):\n",
        "  def __init__(self, input_size, hidden1_size, hidden2_size, num_classes):\n",
        "\n",
        "    super(). __init__()\n",
        "    self.fc1 = nn.Linear(input_size, hidden1_size)\n",
        "    self.relu1 = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(hidden1_size, hidden2_size)\n",
        "    self.relu2 = nn.ReLU()\n",
        "    self.fc3 = nn.Linear(hidden2_size, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    out = self.fc1(x)\n",
        "    out = self.relu1(out)\n",
        "    out = self.fc2(out)\n",
        "    out = self.relu2(out)\n",
        "    out = self.fc3(out)\n",
        "    return out\n",
        "# instantiating a class\n",
        "model = IrisNet(4,100,50,3)\n",
        "print(model)\n",
        "\n",
        "# creating a Dataset objects\n",
        "train_dataset = IrisDataset(X_train, y_train)\n",
        "test_dataset = IrisDataset(X_val, y_val)\n",
        "\n",
        "# creating the DataLoader object\n",
        "batch_size = 60\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Defining the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 250\n",
        "loss_value = []\n",
        "for epoch in range(num_epochs):\n",
        "  #go forward and get a prediction\n",
        "  y_pred = model.forward(X_train)\n",
        "  #measure the loss\n",
        "  loss = criterion(y_pred, y_train)\n",
        "  print(\"loss\", loss)\n",
        "  #keep track of the losses\n",
        "  loss_value.append(loss.detach().numpy())\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()  # Backward pass\n",
        "  optimizer.step()  # Update parameters\"\"\"\n",
        "\n",
        "#Evaluation of the model on the dataset\n",
        "with torch.no_grad(): # this basically turn off the back propagation\n",
        "  y_eval = model.forward(X_val)\n",
        "  loss = criterion(y_eval, y_val)\n",
        "  print(loss)\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "  for i, data in enumerate(X_val):\n",
        "    y_var = model.forward(data)\n",
        "\n",
        "    if y_val[i] == 0:\n",
        "      x = \"Setosa\"\n",
        "    elif y_val[i] ==1:\n",
        "      x = \"Versicolour\"\n",
        "    else:\n",
        "      x = \"Virginica\"\n",
        "    print(f'{i+1}.){str(y_var)}\\t{x}\\t {y_var.argmax().item()}')\n",
        "    #correct or not\n",
        "    if y_var.argmax().item() == y_val[i]:\n",
        "      correct +=1\n",
        "print(f'we got {correct} correct!')\n",
        "\n",
        "\n",
        "\n",
        "# another way the train loop can be written\n",
        "\"\"\"for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        outputs = model(inputs)  # Forward pass\n",
        "        loss = criterion(outputs, labels)  # Compute the loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update parameters\"\"\"\n",
        "\n",
        "# Validation\n",
        "\"\"\"model.eval()  # Set the model to evaluation mode\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():  # Turn off gradient tracking during validation\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)  # Get the predicted class\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "val_accuracy = correct / total\n",
        "print(f'Epoch [{epoch+1}/{num_epochs}], Validation Accuracy: {val_accuracy:.2f}')\"\"\"\n",
        "\n",
        "\n",
        "# Usage:\n",
        "#validation_accuracy = calculate_model_validation(model, test_loader)\n",
        "#print(f'Overall Validation Accuracy: {validation_accuracy:.2f}%')\n",
        "\n",
        "print(\"Training finished.\")\n",
        "# checking for the classification\n",
        "new_iris= torch.tensor([5.4, 3.4, 1.7, 0.2])\n",
        "with torch.no_grad():\n",
        "  print(model(new_iris))\n",
        "#for graphical representation of the model\n",
        "plt.plot(range(num_epochs),loss_value)\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"num_epochs\")\n",
        "#to save the nn Model\n",
        "torch.save(model.state_dict(), 'my_awesome_iris_model.pt')\n",
        "#to load the saved model\n",
        "new_model = IrisNet(input_size=4, hidden1_size=100, hidden2_size=50, num_classes=3)\n",
        "new_model.load_state_dict(torch.load('my_awesome_iris_model.pt'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TYuBm13zqq1k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}